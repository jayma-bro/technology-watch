{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (C:\\Users\\Jayma\\.cache\\huggingface\\datasets\\librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset librispeech_asr (C:\\Users\\Jayma\\.cache\\huggingface\\datasets\\librispeech_asr\\other\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "100%|██████████| 50/50 [05:17<00:00,  6.36s/it]\n",
      "100%|██████████| 50/50 [05:22<00:00,  6.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets.load import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "librispeech_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "librispeech_other = load_dataset(\"librispeech_asr\", \"other\", split=\"test\")\n",
    "sample_size = 50\n",
    "\n",
    "X_clean = []\n",
    "Y_clean = []\n",
    "for ind in tqdm(np.random.randint(0, len(librispeech_clean['audio']), sample_size)):\n",
    "    X_clean.append(librispeech_clean['audio'][ind]['array'])\n",
    "    Y_clean.append(librispeech_clean['text'][ind])\n",
    "    \n",
    "X_other = []\n",
    "Y_other = []\n",
    "for ind in tqdm(np.random.randint(0, len(librispeech_other['audio']), sample_size)):\n",
    "    X_other.append(librispeech_other['audio'][ind]['array'])\n",
    "    Y_other.append(librispeech_other['text'][ind])\n",
    "    \n",
    "data = {\n",
    "    \"X_clean\": X_clean,\n",
    "    \"Y_clean\": Y_clean,\n",
    "    \"X_other\": X_other,\n",
    "    \"Y_other\": Y_other\n",
    "}\n",
    "with open('Data/data_librispeech.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer les prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Data2VecAudioForCTC, Wav2Vec2ForCTC\n",
    "import jiwer\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# load model and processor\n",
    "processor_data2vec = Wav2Vec2Processor.from_pretrained(\"facebook/data2vec-audio-large-960h\")\n",
    "processor_wav2vec = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "mdl_data2vec = Data2VecAudioForCTC.from_pretrained(\"facebook/data2vec-audio-large-960h\")\n",
    "mdl_wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# load data\n",
    "with open('Data/data_librispeech.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "input_data2vec_clean = processor_data2vec(data['X_clean'], return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\").input_values\n",
    "input_wav2vec_clean = processor_wav2vec(data['X_clean'], return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\").input_values\n",
    "input_data2vec_other = processor_data2vec(data['X_other'], return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\").input_values\n",
    "input_wav2vec_other = processor_wav2vec(data['X_other'], return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_data2vec_clean = mdl_data2vec(input_data2vec_clean.float()).logits\n",
    "    logits_wav2vec_clean = mdl_wav2vec(input_wav2vec_clean.float()).logits\n",
    "    logits_data2vec_other = mdl_data2vec(input_data2vec_other.float()).logits\n",
    "    logits_wav2vec_other = mdl_wav2vec(input_wav2vec_other.float()).logits\n",
    "    \n",
    "data['wav2vec_clean'] = processor_wav2vec.batch_decode(torch.argmax(logits_wav2vec_clean, dim=-1))\n",
    "data['data2vec_clean'] = processor_data2vec.batch_decode(torch.argmax(logits_data2vec_clean, dim=-1))\n",
    "data['wav2vec_other'] = processor_wav2vec.batch_decode(torch.argmax(logits_wav2vec_other, dim=-1))\n",
    "data['data2vec_other'] = processor_data2vec.batch_decode(torch.argmax(logits_data2vec_other, dim=-1))\n",
    "\n",
    "with open('Data/data_librispeech.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pickle\n",
    "import jiwer\n",
    "with open('Data/data_librispeech.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER wav2vec2 clean: 7.1% | CER wav2vec2 clean: 2.7%\n",
      "WER data2vec clean: 1.7% | CER data2vec clean: 0.4%\n",
      "WER wav2vec2 other: 7.4% | CER wav2vec2 other: 3.3%\n",
      "WER data2vec other: 3.0% | CER data2vec other: 1.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"WER wav2vec2 clean: {:.1f}% | CER wav2vec2 clean: {:.1f}%\".format(jiwer.wer(data['Y_clean'], data['wav2vec_clean'])*100, jiwer.cer(data['Y_clean'], data['wav2vec_clean'])*100))\n",
    "print(\"WER data2vec clean: {:.1f}% | CER data2vec clean: {:.1f}%\".format(jiwer.wer(data['Y_clean'], data['data2vec_clean'])*100, jiwer.cer(data['Y_clean'], data['data2vec_clean'])*100))\n",
    "print(\"WER wav2vec2 other: {:.1f}% | CER wav2vec2 other: {:.1f}%\".format(jiwer.wer(data['Y_other'], data['wav2vec_other'])*100, jiwer.cer(data['Y_other'], data['wav2vec_other'])*100))\n",
    "print(\"WER data2vec other: {:.1f}% | CER data2vec other: {:.1f}%\".format(jiwer.wer(data['Y_other'], data['data2vec_other'])*100, jiwer.cer(data['Y_other'], data['data2vec_other'])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(len(data['Y_clean'])):\n",
    "    score.append(jiwer.wer(data['Y_other'][i], data['data2vec_other'][i]))\n",
    "score = np.array(score)\n",
    "data['Y_other'] = np.array(data['Y_other'])\n",
    "data['Y_clean'] = np.array(data['Y_clean'])\n",
    "data['wav2vec_other'] = np.array(data['wav2vec_other'])\n",
    "data['wav2vec_clean'] = np.array(data['wav2vec_clean'])\n",
    "data['data2vec_other'] = np.array(data['data2vec_other'])\n",
    "data['data2vec_clean'] = np.array(data['data2vec_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUT THIS SHALL BANISH IT UTTERLY\n",
      "BUT THEY SOUBANISD IT UTTERLY\n",
      "BUT THEY SHALL BANISH IT UTTERLY\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "print(data['Y_other'][score > 0.1][i])\n",
    "print(data['wav2vec_other'][score > 0.1][i])\n",
    "print(data['data2vec_other'][score > 0.1][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
